# -*- coding: utf-8 -*-
"""FinalProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uCOyMIXQWgzn9otD_EeoLo3N5XE6PClj

### Part 1: Data loading
"""

#here's a 12-grade code, mommy

from google.colab import files
import json

! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets download yelp-dataset/yelp-dataset

! unzip yelp-dataset.zip

import pandas as pd

checkin = pd.read_json('yelp_academic_dataset_checkin.json', lines = True)

checkin

tips = pd.read_json('yelp_academic_dataset_tip.json', lines = True)
business = pd.read_json('yelp_academic_dataset_business.json', lines = True)

tips

def large_json_load(jsonfile, limit):
  data_file = open(jsonfile)
  data = []
  i = 0
  for line in data_file:
    if i != limit:
      data.append(json.loads(line))
      i += 1
    else:
      break
  data_file.close() 
  return pd.DataFrame(data)

reviews = large_json_load("yelp_academic_dataset_review.json", 2000000)

reviews

users = large_json_load("yelp_academic_dataset_user.json", 100000)

users.info()

users

reviews.tail()

reviews.info()

business

"""### Part 2: Data cleaning and pre-processing"""

import numpy as np

#excluding businesses without reviews and users who haven't done any reviews 
business = business[business["review_count"] > 0]
users = users[users["review_count"] > 0]

#generating lists of unique business and users IDs
unique_businesses = set(business["business_id"])
unique_users = set(users["user_id"])
print(f"Unique businesses: {len(unique_businesses)}")
print(f"Unique users: {len(unique_users)}")

#Removing the tips that contain users or businesses not present in the unique sets
print(f"Number of tips before processing: {len(tips)}")
tips = tips.loc[tips["business_id"].isin(unique_businesses)]
tips = tips.loc[tips["user_id"].isin(unique_users)]
print(f"Number of tips before processing: {len(tips)}")

#Removing the reviews that contain users or businesses not present in the unique sets
print(f"Number of reviews before processing: {len(reviews)}")

reviews = reviews.loc[reviews['business_id'].isin(unique_businesses)]
reviews = reviews.loc[reviews['user_id'].isin(unique_users)]

print(f"Number of reviews after processing: {len(reviews)}")

#transforming the attributes column containing a dictionary into many columns representing all different attributes 
business = business.join(pd.json_normalize(business['attributes']))

business.info()

#dropping the original attributes column
business.drop('attributes', inplace=True, axis=1)

#"unpacking" operating hours, in the same way the attributes were transformed
business = business.join(pd.json_normalize(business['hours']))

#splitting Monday into two columns, checking out the results
print(business.Monday[2])
business[['Mon_open', 'Mon_close']] = business['Monday'].str.split('-', 1, expand=True)
print(business.Mon_open[2])
print(business.Mon_close[2])

#applying the same logic to the rest of the days
business[['Tue_open', 'Tue_close']] = business['Tuesday'].str.split('-', 1, expand=True)
business[['Wed_open', 'Wed_close']] = business['Wednesday'].str.split('-', 1, expand=True)
business[['Thu_open', 'Thu_close']] = business['Thursday'].str.split('-', 1, expand=True)
business[['Fri_open', 'Fri_close']] = business['Friday'].str.split('-', 1, expand=True)
business[['Sat_open', 'Sat_close']] = business['Saturday'].str.split('-', 1, expand=True)
business[['Sun_open', 'Sun_close']] = business['Sunday'].str.split('-', 1, expand=True)

#dropping the original columns
business.drop(["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"], inplace=True, axis=1)

business.info()

#custom function for replacing string with time
from datetime import datetime
def convert_to_time(time_str):
  try:
    return datetime.strptime(time_str, '%H:%M').time()
  except:
    return None

#checking on two examples: with nan and actual time
print(business.Mon_open[0])
print(convert_to_time(business.Mon_open[0]))
print(business.Mon_open[2])
print(convert_to_time(business.Mon_open[2]))

#creating an array holding the indexes of all of the opening/closing hour features, numbers known from business.info()
days = np.arange(52,66,1)

#looping over every time column and convertig it to time
for column in business.columns[days[0]:days[-1]]:
  business[column] = [convert_to_time(x) for x in business[column]]

#checking if every column has converted correctly
for column in business.columns[days[0]:days[-1]]:
  print(column)
  for x in range(3):
    print(business[column][x])

checkin

#converting date feature into the list, note the items are separated by a comma AND a space
checkin["list_date"] = [x.split(", ") for x in checkin["date"]]

#dropping the original date feature
checkin.drop(["date"], axis = 1, inplace = True)

test = checkin["list_date"][0]

#defining a function to count checkins for a specific year
def count_year(year, time_list):
  counter = 0
  #print(time_list)
  for row in time_list:
      if datetime.strptime(row, '%Y-%m-%d %H:%M:%S').year == year:
        counter += 1
  return counter

print(count_year(2021, test))

#this idea was discarded as the observations were not as helpful as reviews are; users seem to make more reviews than checkins
#moreover, this is a highly memory-consuming process, and it would have to be done for every year, since 2010 to 2022
#checkin["checkins_2015"] = [count_year(2015, x) for x in checkin["list_date"]]

#creating a number of total checkins instead
checkin["No_of_checkins"] = [len(x) for x in checkin["list_date"]]

#removing irrelevant users columns concerned with Yelp profile compliments, as this app is not centered around the users
users = users.drop(users.iloc[:, 11:21],axis = 1)

#importing spaCy library for text preprocessing, including removal of stopwords in a pipeline, setting the language to English
import spacy
nlp = spacy.load('en_core_web_sm')
stopwords = spacy.lang.en.stop_words.STOP_WORDS

#creating a function using spaCy pipeline to process the words 
def business_description_processing(bus_desc):
  description = nlp(bus_desc, disable = ['ner', 'parser'])
  lemmas = [token.lemma_.lower() for token in description]
  alph_and_not_stop = [lemma for lemma in lemmas if lemma.isalpha() and lemma not in stopwords]
  return alph_and_not_stop

#ensuring every row is a string
business["categories"] = business["categories"].astype(str)

#applying the function to return a feature of lists containing cleaned, lemmatised descriptions without stopwords
business["clean_text"] = business["categories"].apply(business_description_processing)

business["clean_text"]

#next steps require processing the cleaned text by putting it into a dictionary
from gensim.matutils import corpus2dense
from gensim.models.tfidfmodel import TfidfModel
from gensim.corpora.dictionary import Dictionary

#creating a dictionary - a mapping of IDs of words and the words in the cleaned texts themselves
dictionary = Dictionary(business['clean_text'])

#creating a corpus counting how many times each word appears in a document with bag of words, meaning what's important is how many times the words occur
corpus = [dictionary.doc2bow(doc) for doc in business['clean_text']]

#Lastly, a term frequencyâ€“inverse document frequency model is created to show the importance of words in a corpus
tf_idf = TfidfModel(corpus)
tf_idf_corpus = tf_idf[corpus]

#importing the LSI model
from gensim.models.lsimodel import LsiModel

#initialising the LSI model with the corpus containing frequency, dictionary to refer to the words, and the amount of topics
#the number of chosen topics results from an iterative process, further described in the paper
lsi_model = LsiModel(tf_idf_corpus, id2word=dictionary, num_topics=3)

#displaying the topics, looks like the dataset consists of the restaurants, spas/beauty saloons, and repair services, very american
lsi_model.show_topics(num_topics=3)

#creating the model corpus
lsi_corpus = lsi_model[tf_idf_corpus]

#for the topic matrix
from gensim.similarities import MatrixSimilarity

#creating the topic matrix
document_topic_matrix = MatrixSimilarity(lsi_corpus)
document_topic_matrix_ix = document_topic_matrix.index

sims = document_topic_matrix[lsi_corpus[0]]
sims = sorted(enumerate(sims), key=lambda item: -item[1])

from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

#reducing the matrix with PCA, n_components argument is defined by the amount of topics created with LSI
reduced = PCA(n_components = 3).fit_transform(document_topic_matrix_ix)
reduced

#Using KMeans clustering to cluster 
clusterer = KMeans(n_clusters = 3, max_iter = 500, n_init = 20, algorithm = "elkan")
clusterer.fit(reduced)

#I think it's shit and can't be shown on the exam, idk
clusterer.inertia_

import seaborn as sns

import matplotlib.pyplot as plt

sns.set_style("darkgrid")

plt.rcParams.update({'font.size': 12})
plt.figure(figsize=(12,12))
graph = sns.scatterplot(reduced[:,0],reduced[:,1], hue=clusterer.labels_, palette="Paired", legend='full')

tfidf_matrix = corpus2dense(tf_idf_corpus, len(dictionary)).T

business["industry"] = clusterer.labels_

for x in range(3):
  print("Cluster",str(x))
  series = business[business["industry"] == x]["categories"]
  print(series)
  print("\n")

clusters_names = {
    0 : "Bars & Restaurants",
    1 : "Repair & Other",
    2 : "Health & Beauty"
}

business["industry"]=business["industry"].map(clusters_names)

business.head(3)

"""### Part 3 : Exploratory Data Analysis (EDA) """

#getting proportions per industry
print(business['industry'].value_counts()) 
plt.xlabel("Industry")
plt.ylabel("Amount")
business['industry'].value_counts().plot(figsize=(7, 6), color = ['steelblue', 'skyblue', 'thistle'], lw=0, kind='bar')

business.head(3)

#summary statistics
business.describe()

#calculate the percentage of stars
business.stars.value_counts(normalize=True)

#plot the pie chart of stars
business.stars.value_counts(normalize=True).plot.pie(figsize=(8, 9),explode=[0.1, 0.1, 0, 0,0,0,0,0,0], autopct='%1.1f%%',
        shadow=True, startangle=0)

#visualising stars reviews per state
sns.catplot(data=business, 
            x='state', 
            y='stars', height = 10, legend= True,
            kind="box")

#adding industry as feature 
plt.figure(figsize=(14,9))

sns.boxplot(data = business, x = "state", y = "stars" , 
            hue = "industry")

tips.head(3)

reviews.head(3)

reviews["weekday"] = [datetime.strptime(x, '%Y-%m-%d %H:%M:%S').strftime("%A") for x in reviews["date"]]

reviews["weekday"].unique()

#bringing weekdays in order HELP DON'T KNOW HOW TO PUT THESE IN 
ordered_weekdays = ['Monday','Tuesday' ,'Wednesday','Thursday','Friday','Saturday','Sunday']

reviews["year"] = [datetime.strptime(x, '%Y-%m-%d %H:%M:%S').strftime("%Y") for x in reviews["date"]]

#note to myself - this plot below can be represented as stacked bar chart accounting percentage of reviews per weekday --> x axis are years

d = reviews.groupby(['year', 'weekday'])["review_id"].count().reset_index()
d = d.rename(columns = {"review_id" : "count"})
d = d.pivot_table("count", ["year"], "weekday")
d = d.reindex(["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"], axis=1)
d.reset_index(inplace=True)
d = d.rename(columns = {'index':'year'})
d

d.describe()

d['Total'] = d['Monday'] + d['Tuesday'] + d['Wednesday'] + d['Thursday'] + d['Friday'] + d['Saturday'] + d['Sunday']

d

d['Sunday'] = np.round((d['Sunday']/ d['Total']), decimals=2)

d['Monday'] = np.round((d['Monday']/ d['Total']), decimals=2)
d['Tuesday'] = np.round((d['Tuesday']/ d['Total']), decimals=2)
d['Wednesday'] = np.round((d['Wednesday']/ d['Total']), decimals=2)
d['Thursday'] = np.round((d['Thursday']/ d['Total']), decimals=2)
d['Friday'] = np.round((d['Friday']/ d['Total']), decimals=2)
d['Saturday'] = np.round((d['Saturday']/ d['Total']), decimals=2)
d['Sunday'] = np.round((d['Sunday']/ d['Total']), decimals=2)

plt.rcParams["figure.figsize"] = (16,12)

ax = d.plot(x="year", y=["Monday", "Tuesday", "Wednesday", 'Thursday', 'Friday', 'Saturday', 'Sunday'], kind="bar", rot=0, stacked=True)
_ = ax.legend(bbox_to_anchor=(1, 1), loc='upper left')
plt.xlabel("Year")
plt.ylabel("Proportional Star Review in %")

d.describe()

business.info()

business.drop(["DietaryRestrictions", "AgesAllowed", "RestaurantsCounterService", "Open24Hours", "HairSpecializesIn", "BYOBCorkage", "Corkage", "BYOB", "BestNights", "AcceptsInsurance", "GoodForDancing", "Music", "Ambience", "GoodForMeal", "Smoking", "DriveThru", "CoatCheck"], inplace=True, axis=1)

business.drop("hours", inplace = True, axis = 1)

business["BusinessParking"] = [1 if type(x) == str and x != "None" else 0 for x in business["BusinessParking"]]

business["WiFi"].unique()

business["WiFi"] = [1 if x == "u'free'" or x == "u'paid'" or x == "'free'" or x == "'paid'" else 0 for x in business["WiFi"]]

business["WiFi"] = [1 if x == "u'free'" or x == "u'paid'" or x == "'free'" or x == "'paid'" else 0 for x in business["WiFi"]]

business["Alcohol"] = [1 if x in ["u'full_bar'", "'full_bar'", "u'beer_and_wine'", "'beer_and_wine'"] else 0 for x in business["Alcohol"]]

business["RestaurantsAttire"] = ["casual" if x in ["u'casual'", "'casual'"] else "formal" if x in ["u'formal'", "'formal'"] else "dressy" if x in ["'dressy'", "u'dressy'"] else "none" for x in business["RestaurantsAttire"]]

business["NoiseLevel"] = [0 if x == "None" else 1 if x in ["'quiet'", "u'quiet'"] else 2 if x in ["u'average'", "'average'"] else 3 if x in ["u'loud'", "'loud'"] else 4 if x in ["'very_loud'", "u'very_loud'"] else 0 for x in business["NoiseLevel"]]

business["RestaurantAttire"].unique()

business["ByAppointmentOnly"] = [1 if x == "True" else 0 for x in business["ByAppointmentOnly"]]
business["BusinessAcceptsCreditCards"] = [1 if x == "True" else 0 for x in business["BusinessAcceptsCreditCards"]]
business["BikeParking"] = [1 if x == "True" else 0 for x in business["BikeParking"]]
business["RestaurantsTakeOut"] = [1 if x == "True" else 0 for x in business["RestaurantsTakeOut"]]
business["RestaurantsDelivery"] = [1 if x == "True" else 0 for x in business["RestaurantsDelivery"]]
business["Caters"] = [1 if x == "True" else 0 for x in business["Caters"]]
business["WheelchairAccessible"] = [1 if x == "True" else 0 for x in business["WheelchairAccessible"]]
business["HappyHour"] = [1 if x == "True" else 0 for x in business["HappyHour"]]
business["OutdoorSeating"] = [1 if x == "True" else 0 for x in business["OutdoorSeating"]]
business["HasTV"] = [1 if x == "True" else 0 for x in business["RestaurantsReservations"]]
business["RestaurantsReservations"] = [1 if x == "True" else 0 for x in business["WheelchairAccessible"]]
business["DogsAllowed"] = [1 if x == "True" else 0 for x in business["DogsAllowed"]]
business["GoodForKids"] = [1 if x == "True" else 0 for x in business["GoodForKids"]]
business["RestaurantsTableService"] = [1 if x == "True" else 0 for x in business["RestaurantsTableService"]]
business["RestaurantsGoodForGroups"] = [1 if x == "True" else 0 for x in business["RestaurantsGoodForGroups"]]
business["BusinessAcceptsBitcoin"] = [1 if x == "True" else 0 for x in business["BusinessAcceptsBitcoin"]]

price_range = business["RestaurantsPriceRange2"].to_numpy()
price = 0
counter = 0
for x in range(len(price_range)):
  try:
    int(price_range[x])
  except:
    continue
  price = price + int(price_range[x])
  counter = counter + 1
average = int(np.round(price/counter))
print(price)
print(counter)
print(average)

business["RestaurantsPriceRange2"] = business["RestaurantsPriceRange2"].fillna(average)

business["RestaurantsPriceRange2"] = [average if x == "None" else int(x) for x in business["RestaurantsPriceRange2"]]
business = business.rename(columns={"RestaurantsPriceRange2": "PriceRange"})

business.iloc[ : , 12:36].head(3)

"""### Part 4: Reviews topic modelling"""

reviews = reviews.merge(business[["city", "state", "industry", "business_id"]], how = "left",

                left_on = "business_id", right_on = "business_id").drop(columns = ["business_id"])

reviews.columns

#storing column features as string by converting to list
reviews['text_clean'] = reviews['text'].tolist()

reviews.head(3)

#preparation of textual data for LDA analysis 
#removing punctuation and lowercasing 
# Loading regular expression library
import re

# Remove punctuation
reviews['text_clean'] = \
reviews['text'].map(lambda x: re.sub(r'[,\/.!?-]', '', x)) 


# Convert the titles to lowercase
reviews['text_clean'] = \
reviews['text_clean'].map(lambda x: x.lower())

reviews['text_clean'].head()

#import additional packages & modules 
import gensim
from gensim.utils import simple_preprocess
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = stopwords.words('english') #usage of english stop words 

#stop_words.extend(['avengers','shalimar'])

def sent_to_words(sentences):
    for sentence in sentences:
        # deacc=True removes punctuations
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))

def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) 
             if word not in stop_words] for doc in texts]

data_tips = reviews.text_clean.values.tolist()
data_words = list(sent_to_words(data_tips))


# remove stop words
data_words = remove_stopwords(data_words)

print(data_words[:1][0][:30])

#importing module to create gensim corpus
import gensim.corpora as corpora

#create dictionaries 
id2word = corpora.Dictionary(data_words)


#create Corpus
texts_ = data_words

#Term Frequency
corpus = [id2word_reviews.doc2bow(text) for text in texts_]

#LDA model training
from pprint import pprint
#number of topics
num_topics = 3
# Build LDA model
lda_model = gensim.models.LdaMulticore(corpus=corpus,
                                       id2word=id2word,
                                       num_topics=num_topics)

# Print keywords in the 3 topics of reviews
pprint(lda_model.print_topics())
doc_lda = lda_model[corpus]

#analysis of LDA model result's
# Import pyLDAvis

!pip install -qq pyLDAvis
import pyLDAvis
import pyLDAvis.gensim_models as gensimvis

#preparation for visualization
lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)

#visualization  tips 
pyLDAvis.display(lda_display)

"""### Part 5: Predictive analysis

####Part 5.1: Further data processing
"""

#importing all of the necessary sklearn's modules
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score, KFold
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

business.info()

print(f"{business.Tue_open[0]}\n{business.Tue_close[1]}")

#adding binary columns based on the opening hours from part 2
business["Monday"] = [0 if business.Mon_open[x] == None and business.Mon_close[x] == None else 1 for x in range(len(business))]
business["Tuesday"] = [0 if business.Tue_open[x] == None and business.Tue_close[x] == None else 1 for x in range(len(business))]
business["Wednesday"] = [0 if business.Wed_open[x] == None and business.Wed_close[x] == None else 1 for x in range(len(business))]
business["Thursday"] = [0 if business.Thu_open[x] == None and business.Thu_close[x] == None else 1 for x in range(len(business))]
business["Friday"] = [0 if business.Fri_open[x] == None and business.Fri_close[x] == None else 1 for x in range(len(business))]
business["Saturday"] = [0 if business.Sat_open[x] == None and business.Sat_close[x] == None else 1 for x in range(len(business))]
business["Sunday"] = [0 if business.Sun_open[x] == None and business.Sun_close[x] == None else 1 for x in range(len(business))]

#dropping the old weekday columns
business.drop(["Mon_open", "Mon_close", "Tue_open", "Tue_close", "Wed_open", "Wed_close", "Thu_open", "Thu_close", "Fri_open", "Fri_close", "Sat_open", "Sat_close", "Sun_open", "Sun_close"], inplace=True, axis=1)

business.info()

#dropping the features where null values are a vast majority
business.drop(["DietaryRestrictions", "AgesAllowed", "RestaurantsCounterService", "Open24Hours", "HairSpecializesIn", "BYOBCorkage", "Corkage", "BYOB", "BestNights", "AcceptsInsurance", "GoodForDancing", "Music", "Ambience", "GoodForMeal", "Smoking", "DriveThru", "CoatCheck"], inplace=True, axis=1)

#dropping the hours too
business.drop("hours", inplace = True, axis = 1)

#encoding the columns using list comprehensions, mostly 1 for "True", 0 for everything else
#it was assumed "None" or nan values are equal to 0
business["BusinessParking"] = [1 if type(x) == str and x != "None" else 0 for x in business["BusinessParking"]]

#some features have more values than "True", "Falase", "None" and nan, like this WiFi
business["WiFi"].unique()

business["WiFi"] = [1 if x == "u'free'" or x == "u'paid'" or x == "'free'" or x == "'paid'" else 0 for x in business["WiFi"]]

business["Alcohol"] = [1 if x in ["u'full_bar'", "'full_bar'", "u'beer_and_wine'", "'beer_and_wine'"] else 0 for x in business["Alcohol"]]

business["RestaurantsAttire"] = ["casual" if x in ["u'casual'", "'casual'"] else "formal" if x in ["u'formal'", "'formal'"] else "dressy" if x in ["'dressy'", "u'dressy'"] else "none" for x in business["RestaurantsAttire"]]

business["NoiseLevel"] = [0 if x == "None" else 1 if x in ["'quiet'", "u'quiet'"] else 2 if x in ["u'average'", "'average'"] else 3 if x in ["u'loud'", "'loud'"] else 4 if x in ["'very_loud'", "u'very_loud'"] else 0 for x in business["NoiseLevel"]]

business["ByAppointmentOnly"] = [1 if x == "True" else 0 for x in business["ByAppointmentOnly"]]
business["BusinessAcceptsCreditCards"] = [1 if x == "True" else 0 for x in business["BusinessAcceptsCreditCards"]]
business["BikeParking"] = [1 if x == "True" else 0 for x in business["BikeParking"]]
business["RestaurantsTakeOut"] = [1 if x == "True" else 0 for x in business["RestaurantsTakeOut"]]
business["RestaurantsDelivery"] = [1 if x == "True" else 0 for x in business["RestaurantsDelivery"]]
business["Caters"] = [1 if x == "True" else 0 for x in business["Caters"]]
business["WheelchairAccessible"] = [1 if x == "True" else 0 for x in business["WheelchairAccessible"]]
business["HappyHour"] = [1 if x == "True" else 0 for x in business["HappyHour"]]
business["OutdoorSeating"] = [1 if x == "True" else 0 for x in business["OutdoorSeating"]]
business["HasTV"] = [1 if x == "True" else 0 for x in business["RestaurantsReservations"]]
business["RestaurantsReservations"] = [1 if x == "True" else 0 for x in business["WheelchairAccessible"]]
business["DogsAllowed"] = [1 if x == "True" else 0 for x in business["DogsAllowed"]]
business["GoodForKids"] = [1 if x == "True" else 0 for x in business["GoodForKids"]]
business["RestaurantsTableService"] = [1 if x == "True" else 0 for x in business["RestaurantsTableService"]]
business["RestaurantsGoodForGroups"] = [1 if x == "True" else 0 for x in business["RestaurantsGoodForGroups"]]
business["BusinessAcceptsBitcoin"] = [1 if x == "True" else 0 for x in business["BusinessAcceptsBitcoin"]]

#imputing the places with no price range by calculating the mean of the places with filled information
price_range = business["RestaurantsPriceRange2"].to_numpy()
price = 0
counter = 0
#if a value can be switched into an integer (e.g. "1", "2" or "3") it's added to the total price and counter variables
for x in range(len(price_range)):
  try:
    int(price_range[x])
  except:
    continue
  price = price + int(price_range[x])
  counter = counter + 1
average = int(np.round(price/counter))
print(price)
print(counter)
print(average)

#filling na values with the calculated average
business["RestaurantsPriceRange2"] = business["RestaurantsPriceRange2"].fillna(average)

#also for "None", changing the feature name to make it look more general rather than only for restaurants and bars
business["RestaurantsPriceRange2"] = [average if x == "None" else int(x) for x in business["RestaurantsPriceRange2"]]
business = business.rename(columns={"RestaurantsPriceRange2": "PriceRange"})

business.iloc[ : , 12:36].head(3)

#creating the dummies from the two categorical features: attire and industry
business = pd.get_dummies(business, columns = ["RestaurantsAttire", "industry"])

"""####Part 5.2: Calculating features correlation"""

business.info()

#plotting a correlation coefficients heatmap
plt.figure(figsize=(40, 40))

heatmap = sns.heatmap(business.corr(), vmin=-1, vmax=1, annot=True)
heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);

#creating a dataframe containing the features correlation with the stars review
stars_corr = pd.Series.to_frame(business.corr()["stars"])
stars_corr = stars_corr.rename_axis("feature").reset_index()
stars_corr = stars_corr[stars_corr.feature != "stars"]
stars_corr = stars_corr[stars_corr.stars.isna() != True]
stars_corr = stars_corr.sort_values(by = ["stars"])

#plotting the above dataframe
plt.figure(figsize=(20,15))
sns.barplot(data = stars_corr, x = "stars", y = "feature", orient = "h")
plt.title("Correlation of business attributes and stars rating")
plt.xlabel("correlation coefficient")

#X holds explanatory variables, dropping categoricals and stars, y holds dependant variable
X = business.drop(["stars", "business_id", "name", "address", "city", "state", "postal_code", "categories", "clean_text"], axis=1).values
y = business["stars"].values

#scaling the features
X =  StandardScaler().fit_transform(X)

#splitting the data into train and test sets, shuffle set to True to reduce the possibility of overfitting, random state to be able to reproduce the results
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle = True, random_state = 11)

#creating three regressions and one ensemble method
netreg = ElasticNet()
lassoreg = Lasso()
ridgreg = Ridge()
forest = RandomForestRegressor()

#fitting the training data to the models
netreg.fit(X_train, y_train)
lassoreg.fit(X_train, y_train)
ridgreg.fit(X_train, y_train)

#retrieving the scores
print(f"Elastic Net regression score: {netreg.score(X_test, y_test)}")
print(f"Lasso regression score: {lassoreg.score(X_test, y_test)}")
print(f"Ridge regression score: {ridgreg.score(X_test, y_test)}")

#predicting the numbers with the trained models
Net_pred = netreg.predict(X_test)
Las_pred = lassoreg.predict(X_test)
Rid_pred = ridgreg.predict(X_test)

#calculating RMSE with the predicted values vs test data
print(mean_squared_error(y_test, Net_pred, squared=False))
print(mean_squared_error(y_test, Las_pred, squared=False))
print(mean_squared_error(y_test, Rid_pred, squared=False))

"""#### Part 5.3: Hyperparameter tuning"""

#setting up the dictionaries of parameters for the regression models
net_hyper = {"alpha" : np.linspace(0.00001, 1, 20), 
             "l1_ratio": np.linspace(0, 1, 20)}

las_hyper = {"alpha" : np.linspace(0.00001, 1, 20)}

rid_hyper = {"alpha" : np.linspace(0.00001, 1, 20),
             "solver" : ["auto", "svd", "cholesky", "lsqr", "sparse_cg", "sag", "saga", "lbfgs"]}

#creating a 5fold data splits, shuffle = True for more randomness, random state to reproduce the results
kf = KFold(n_splits = 5, shuffle = True, random_state = 11)

#creating a dictionary of models
models_dict = {"Lasso regression": Lasso(), "Ridge regression": Ridge(), "Elastic Net regression": ElasticNet()}

#creating a list to store all of the results
results = []

#for each model, calculate validate scores with 5folds, append the results list with the scores
for model in models_dict.values():
  cv_results = cross_val_score(model, X_train, y_train, cv= kf)
  results.append(cv_results)

#plotting the results on a box plot
plt.figure(figsize=(8,6))
plt.boxplot(results, labels=models_dict.keys())
plt.title("Regression models before hyperparameter optimisation")
plt.show()

#importing time to work calculate the processing time difference of grid and randomized searches
import time

#an experiment: seeing whether grid or randomised returns better results
#setting up a grid search with hyperparameters for Lasso
lasso_cv = GridSearchCV(Lasso(), las_hyper, cv = kf)

#fitting a lasso model with Grid search
start = time.time()
lasso_cv.fit(X_train, y_train)
stop = time.time()
grid_time = stop - start

#retrieving the best parameters and score from the training
grid_best_param = lasso_cv.best_params_
grid_best_score = lasso_cv.best_score_

#same procedure as above but with randomized search instead
random_lasso_cv = RandomizedSearchCV(Lasso(), las_hyper, cv = kf)

start = time.time()
random_lasso_cv.fit(X_train, y_train)
stop = time.time()
random_time = stop - start

random_best_param = random_lasso_cv.best_params_
random_best_score = random_lasso_cv.best_score_

#printing the differences between the best parameters, scores and times of processing
print(f"Randomised search best parameter for Lasso: {random_best_param}")
print(f"Randomised search best score for Lasso: {random_best_score}")
print(f"Grid search best parameter for Lasso: {grid_best_param}")
print(f"Grid search best score for Lasso: {grid_best_score}")
print(f"Standard deviation of the scores: {np.std([random_best_score, grid_best_score])}")
print(f"Time for training using the random search: {random_time}s")
print(f"Time for training using the grid search: {grid_time}")

#since there are no differences, the models will be tuned using randomised search
Lasso_cvR = RandomizedSearchCV(Lasso(), las_hyper, cv = kf)
Ridge_cvR = RandomizedSearchCV(Ridge(), rid_hyper, cv = kf)
ElNet_cvR = RandomizedSearchCV(ElasticNet(), net_hyper, cv = kf)

#creating a similar box plot to the one before showing the scores of the models, starting with a dictionary with the tuned models
hyper_models_dict = {"Lasso regression": Lasso_cvR, "Ridge regression": Ridge_cvR, "Elastic Net regression": ElNet_cvR}

#list for results
new_results = []

#looping through the dictionary and appending the list with the results
for model in hyper_models_dict.values():
  cv_results = cross_val_score(model, X_train, y_train, cv= kf)
  new_results.append(cv_results)

#plotting the new box plot
plt.figure(figsize=(8,6))
plt.boxplot(new_results, labels=hyper_models_dict.keys())
plt.title("Regression models after hyperparameter optimisation")
plt.show()

#training the three models
Lasso_cvR.fit(X_train, y_train)
Ridge_cvR.fit(X_train, y_train)
ElNet_cvR.fit(X_train, y_train)

#printing the new scores
print(f"Lasso regression's best score after hyperparameter optimisation is {Lasso_cvR.best_score_}")
print(f"Ridge regression's best score after hyperparameter optimisation is {Ridge_cvR.best_score_}")
print(f"Elastic Net regression's best score after hyperparameter optimisation is {ElNet_cvR.best_score_}")

#getting the parameters for the best Ridge and the RMSE
print(Ridge_cvR.best_params_)
RidgeH_pred = Ridge_cvR.predict(X_test)
print(mean_squared_error(y_test, RidgeH_pred, squared=False))

#to see if there's a possibility of getting a better results, random forest regressor model is also implemented
forest.fit(X_train, y_train)

#it has a better score without any parameters tuned
print(f"Random forest regressor score: {forest.score(X_test, y_test)}")

#predicting values
For_pred = forest.predict(X_test)

#RMSE is barely any better despite the better score
print(mean_squared_error(y_test, For_pred, squared=False))

#RFR hyperparameters dictionary
for_hyper = {'bootstrap': [True, False], 'max_depth': [10, 20, None], 'min_samples_split': [2, 5, 10], 'n_estimators': [25, 50]}

#tuning the parameters with the randomised search CV
forest_cvR = RandomizedSearchCV(RandomForestRegressor(), for_hyper, cv = kf)

#training the model, this process takes over 22 minutes in Google Colab, pleaserun this cell with caution
forest_cvR.fit(X_train, y_train)

#getting the best parameters and score
print(forest_cvR.best_params_)
print(forest_cvR.best_score_)

#predicting values
ForH_pred = forest_cvR.predict(X_test)

#RMSE is not that better considering the computational power random forest regressor requires
print(mean_squared_error(y_test, ForH_pred, squared=False))

"""### Part 6: Building the application"""

#installing and loading streamlit
#!pip install streamlit
import streamlit as st

#setting page configuration
st.set_page_config(layout="centered", page_icon="ðŸ‘”", page_title="Yelp Business Tool")

#setting title and header
st.title("Yelp review-based tool for wannabe entrepreneurs")
st.header("Please enter the data about your business idea to receive the possible average of users reviews and issues you should focus the most when opening the business.")

#asking the questions to get the business data
u_name = st.text_input("Type in your business name")

u_industry = st.selectbox("Type of industry:", ["Bars & Restaurants", "Health & Beauty", "Repair & Other"])

u_state = st.selectbox("State:", [set(business["state"])])

u_city = st.selectbox("City:", [set(business["city"])])

u_address = st.text_input("Type in the address")

u_pricerange = st.slider('Choose your price range on scale 1 to 4', 1, 4)

u_dresscode = st.selectbox("Type of required dresscode:", ["none", "formal", "dressy", "casual"])

u_Mon = st.radio('Is your place open on Mondays? (1 for yes, 0 for no)', [1, 0])
u_Tue = st.radio('Is your place open on Tuesdays? (1 for yes, 0 for no)', [1, 0])
u_Wed = st.radio('Is your place open on Wednesdays? (1 for yes, 0 for no)', [1, 0])
u_Thu = st.radio('Is your place open on Thursdays? (1 for yes, 0 for no)', [1, 0])
u_Fri = st.radio('Is your place open on Fridays? (1 for yes, 0 for no)', [1, 0])
u_Sat = st.radio('Is your place open on Saturday? (1 for yes, 0 for no)', [1, 0])
u_Sun = st.radio('Is your place open on Sundays? (1 for yes, 0 for no)', [1, 0])

u_appointment = st.radio('Does your business accept customers only with appointments? (1 for yes, 0 for no)', [1, 0])
u_cc = st.radio('Does your business accept credit cards? (1 for yes, 0 for no)', [1, 0])
u_bike = st.radio('Does your business have a bike parking? (1 for yes, 0 for no)', [1, 0])
u_takeout = st.radio('Does your business offer takeouts? (1 for yes, 0 for no)', [1, 0])
u_delivery = st.radio('Does your business offer delivery? (1 for yes, 0 for no)', [1, 0])
u_catering = st.radio('Does your business offer catering? (1 for yes, 0 for no)', [1, 0])
u_wifi = st.radio('Does your business have a WiFi for guests? (1 for yes, 0 for no)', [1, 0])
u_business_parking = st.radio('Does your business have a business parking? (1 for yes, 0 for no)', [1, 0])
u_wheelchair = st.radio('Can you business be accessed on a wheelchair? (1 for yes, 0 for no)', [1, 0])
u_happyh = st.radio('Does your business offer happy hours? (1 for yes, 0 for no)', [1, 0])
u_outdoor = st.radio('Does your business have outdoor seating? (1 for yes, 0 for no)', [1, 0])

u_tv = st.radio('Does your business have a TV? (1 for yes, 0 for no)', [1, 0])
u_reserve = st.radio('Does your business allow for reservations? (1 for yes, 0 for no)', [1, 0])
u_dogs = st.radio('Does your business allow dogs inside? (1 for yes, 0 for no)', [1, 0])
u_alc = st.radio('Does your business offer alcohol? (1 for yes, 0 for no)', [1, 0])
u_kid = st.radio('Is your business good for kids? (1 for yes, 0 for no)', [1, 0])
u_waiter = st.radio('Does your business have a waiter/server? (1 for yes, 0 for no)', [1, 0])
u_group = st.radio('Is your business good for groups? (1 for yes, 0 for no)', [1, 0])
u_noiserange = st.slider('Choose the noise level in your business', 0, 4)
u_btc = st.radio('Does your business business accept Bitcoins? (1 for yes, 0 for no)', [1, 0])

def predict_review():
  #options for categorical features
  bar_ind = 0
  health_ind = 0
  repair_ind = 0
  none_att = 0
  form_att = 0
  dres_att = 0
  casu_att = 0
  #choosing a proper dummy depending on the choice of dresscode
  if u_dresscode == "formal":
    form_att = 1
  elif u_dresscode == "dressy":
    dres_att = 1
  elif u_dresscode == "casual":
    casu_att = 1
  else:
    none_att = 1
  #choosing a proper dummy depending on the choice of industry
  if u_industry == "Bars & Restaurants":
    bar_ind = 1
  elif u_industry == "Health & Beauty":
    health_ind = 1
  else:
    repair_ind = 1
  #putting data into an array
  X_input = np.array([0,0,0,1,u_appointment, u_cc, u_bike, u_pricerange, u_takeout, u_delivery, u_catering, u_wifi, u_business_parking, 
           u_wheelchair, u_happyh, u_outdoor, u_tv, u_reserve, u_dogs, u_alc, u_kid, u_waiter, u_group, u_noiserange, u_btc,
           u_Mon, u_Tue, u_Wed, u_Thu, u_Fri, u_Sat, u_Sun, casu_att, dres_att, form_att, none_att, bar_ind, health_ind, repair_ind]).reshape(1, -1)
  #scaling the data
  X_input = StandardScaler().fit_transform(X_input)
  #predicting the data using the model
  u_prediction = Ridge_cvR.predict(X_input)
  #rounding the prediction
  u_prediction = np.round(u_prediction[0])
  return u_prediction

def get_reviews_topics(state, city, industry, pricerange):
  reviews_f = reviews[reviews["industry"] == industry & reviews["city"] == city & reviews["state"] == state & industry["PriceRange"] == pricerange]
  reviews_f['text_clean'] = reviews_f['text'].tolist()
  reviews_f['text_clean'] = \
  reviews_f['text'].map(lambda x: re.sub(r'[,\/.!?-]', '', x))
  reviews_f['text_clean'] = \
  reviews_f['text_clean'].map(lambda x: x.lower())
  nltk.download('stopwords')
  stop_words = stopwords.words('english')
  data_tips = reviews_f.text_clean.values.tolist()
  data_words = list(sent_to_words(data_tips))
  data_words = remove_stopwords(data_words)
  id2word = corpora.Dictionary(data_words)
  texts_ = data_words
  corpus = [id2word.doc2bow(text) for text in texts_]
  num_topics = 3
  lda_model = gensim.models.LdaMulticore(corpus=corpus, id2word=id2word, num_topics=num_topics)
  pprint(lda_model.print_topics())
  doc_lda = lda_model[corpus]
  lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)

#this code creates a button, that once pressed, makes the prediction
if st.button('Predict rating'):
    u_prediction = predict_review()
    st.success(f'Your predicted rating for your business {u_name} is {u_prediction}')

#this code creates a button, that once pressed, generates the topics
if st.button("See topics"):
  st.text("These are the topics that are particularly relevant for your type of business:")
  get_reviews_topics(u_state, u_city, u_industry, u_pricerange)